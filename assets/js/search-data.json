{
  
    
        "post0": {
            "title": "A Monte Carlo Simulation Workflow in Python",
            "content": "Introduction . This is a workflow for Monte Carlo Simulation in Python, using a dataframe to track the parameters, simulated dataset, and summary statistics / model fitting. For multi-core machines, we can try to speed up using parallel computation. . This workflow is inspired by R&#39;s purrr package and the notion of list-columns. I also benefited from Alex Hayes&#39; great note on many models workflow in python. . Running example in this note: . Simulate many of datasets from each of the 9 normal distribution, with 3 different centers and 3 different scales. | Calculate the sample statistics for each of the simulated datasets. | Put everything in a dataframe. | The workflow consists of 4 steps: . Set the model parameters | Simulate datasets according to the parameters | Do stuff with the simulated datasets, e.g., calculate sample statistics, fit models. | Collect steps 1--3 in a dataframe. The main benefit of keeping everything in a dataframe is for later analysis and visualization of the Monte Carlo simulation. | from platform import python_version print(python_version()) . 3.7.8 . import concurrent.futures from itertools import product from typing import Dict, List, Union import numpy as np import pandas as pd . import psutil n_cores = psutil.cpu_count() n_cores . 16 . Set model parameters . N_REP = 1000 # sample size can also be a parameter of simulation, but set to 1 value for illustration SAMPLE_SIZE = [10_000] locations = [0, 1, 2] scales = [1, 2, 3] params = pd.DataFrame( list(product(locations, scales, range(N_REP), SAMPLE_SIZE)), columns=[&quot;loc&quot;, &quot;scale&quot;, &quot;rep&quot;, &quot;size&quot;], ) params . loc scale rep size . 0 0 | 1 | 0 | 10000 | . 1 0 | 1 | 1 | 10000 | . 2 0 | 1 | 2 | 10000 | . 3 0 | 1 | 3 | 10000 | . 4 0 | 1 | 4 | 10000 | . ... ... | ... | ... | ... | . 8995 2 | 3 | 995 | 10000 | . 8996 2 | 3 | 996 | 10000 | . 8997 2 | 3 | 997 | 10000 | . 8998 2 | 3 | 998 | 10000 | . 8999 2 | 3 | 999 | 10000 | . 9000 rows × 4 columns . Simulate datasets . For small dataset, we can keep the simulated data sets in the tracking dataframe. If the simulated dataset is large, we only keep the sample statistics in the dataframe. . def simulate_data_seq(params: pd.DataFrame) -&gt; pd.DataFrame: rng = np.random.default_rng() sim_data = pd.DataFrame() sim_data[&quot;sample&quot;] = [ rng.normal(loc=loc, scale=scale, size=size) for loc, scale, size in zip(params[&quot;loc&quot;], params[&quot;scale&quot;], params[&quot;size&quot;]) ] return sim_data . %%time sim_data_seq = simulate_data_seq(params) . CPU times: user 1.34 s, sys: 177 ms, total: 1.51 s Wall time: 1.52 s . Do stuff with the simulated datasets . def calculate_sample_stats(arr: np.ndarray) -&gt; Dict: &quot;&quot;&quot;calculate sample statistics&quot;&quot;&quot; average = arr.mean() variance = arr.var(ddof=1) p05 = np.quantile(arr, 0.05) p95 = np.quantile(arr, 0.95) return { &quot;average&quot;: average, &quot;variance&quot;: variance, &quot;p05&quot;: p05, &quot;p95&quot;: p95, } def compute_seq(df: pd.DataFrame) -&gt; pd.DataFrame: sample_stats = [calculate_sample_stats(sample) for sample in df[&quot;sample&quot;]] return pd.DataFrame.from_records(sample_stats) . %%time sample_stats_seq = compute_seq(sim_data_seq) . CPU times: user 3.32 s, sys: 12.3 ms, total: 3.34 s Wall time: 3.34 s . Keep everything in a dataframe . pd.concat([params, sim_data_seq, sample_stats_seq], axis=&quot;columns&quot;) . loc scale rep size sample average variance p05 p95 . 0 0 | 1 | 0 | 10000 | [0.319077282511964, 0.18965678964333085, -0.63... | -0.018138 | 1.000294 | -1.665284 | 1.624925 | . 1 0 | 1 | 1 | 10000 | [1.0530038923608775, 0.6534644477062702, 1.385... | -0.006782 | 1.008830 | -1.632020 | 1.693900 | . 2 0 | 1 | 2 | 10000 | [0.619682511046317, 0.8076221364639493, -0.212... | -0.003659 | 0.977135 | -1.634407 | 1.627604 | . 3 0 | 1 | 3 | 10000 | [-0.7515129763643353, -0.8428228923508118, -0.... | 0.007276 | 0.994398 | -1.652222 | 1.654156 | . 4 0 | 1 | 4 | 10000 | [-0.3187877346174399, -0.09063484844832305, -0... | 0.001544 | 1.005466 | -1.639467 | 1.663695 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 8995 2 | 3 | 995 | 10000 | [4.459473155059797, 7.17871505746818, 2.080059... | 1.981215 | 9.112403 | -3.050421 | 6.898461 | . 8996 2 | 3 | 996 | 10000 | [1.3252498963188968, -7.932355926994303, 5.374... | 1.984005 | 8.874890 | -3.008920 | 6.929186 | . 8997 2 | 3 | 997 | 10000 | [-5.566903275579494, 4.783448759876334, -1.454... | 1.999839 | 8.816550 | -2.910886 | 6.922024 | . 8998 2 | 3 | 998 | 10000 | [4.783361765266323, 0.06270761044701434, 6.484... | 2.014064 | 8.844529 | -2.876956 | 6.945541 | . 8999 2 | 3 | 999 | 10000 | [3.597228274125008, 1.7495518614255168, 6.9812... | 1.958327 | 8.951227 | -2.915028 | 6.874206 | . 9000 rows × 9 columns . Parallel computation . We may be able to speed up the simulation and computation using parallel computation, e.g., multiprocessing, multithreading, or other packages, depending on whether the bottlenecks are the I/O, memory, cache, or CPU. . Below is an example of multiprocessing. A few notes when using multiprocessing in Python. . Depending on the size of the datasets, number of datasets, and how cpu-hungry the computations are, multiprocessing may not perform better than sequential processing. In my working example, multiprocessing did not improve performance. In my real simulation work with heavier computations, multiprocessing does help. | We need to change the simulation function a little in terms how it takes in arguments, to facilitate the map function. | Chunksize may be a factor for performance. | Numpy, Scipy, and Pandas scientific computing packages may not play well with multiprocessing. Read more here. | . CHUNKSIZE = int(params.shape[0] / (n_cores / 2)) CHUNKSIZE . 1125 . def simulate_data(params: Dict) -&gt; np.ndarray: loc = params[&quot;loc&quot;] # float scale = params[&quot;scale&quot;] # float size = params[&quot;size&quot;] # int # be sure to reseed the RNG # see discussions here: https://github.com/numpy/numpy/issues/9650 rng = np.random.default_rng() return rng.normal(loc=loc, scale=scale, size=size) def simulate_data_mp(params: List) -&gt; Dict: params_list = params.to_dict(orient=&quot;records&quot;) sim_data = pd.DataFrame() with concurrent.futures.ProcessPoolExecutor() as executor: sim_data[&quot;sample&quot;] = list( executor.map(simulate_data, params_list, chunksize=CHUNKSIZE) ) return sim_data . %%time sim_data_mp = simulate_data_mp(params) . CPU times: user 619 ms, sys: 4.15 s, total: 4.77 s Wall time: 6.22 s . def compute_mp(df: pd.DataFrame) -&gt; pd.DataFrame: with concurrent.futures.ProcessPoolExecutor() as executor: sample_stats = list( executor.map(calculate_sample_stats, df[&quot;sample&quot;], chunksize=CHUNKSIZE) ) return pd.DataFrame.from_records(sample_stats) . %%time sample_stats_mp = compute_mp(sim_data_seq) . CPU times: user 403 ms, sys: 683 ms, total: 1.09 s Wall time: 5.13 s .",
            "url": "https://yimingli.net/a-monte-carlo-simulation-workflow-in-python.html",
            "relUrl": "/a-monte-carlo-simulation-workflow-in-python.html",
            "date": " • Nov 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Differential Privacy for Social Scientists",
            "content": "This year’s NBER (National Bureau of Economic Research) Summer Institute Methods Lecture is on Differential Privacy. . Sessions: . Differential Privacy: Observations for Economists – Daniel Goroff: video, slides | Introduction to Differential Privacy – Daniel Kifer: video, slides | Decisions with Privacy-Protected Data – Ian Schmutte: video, slides | Basic Differential Privacy Algorithms and Statistics – Daniel Kifer: video, slides | Formal Privacy in Census Data – Ian Schmutte: video, slides | Implications of Data Privacy Concerns for Empirical Social Science – Frauke Kreuter: video, paper | .",
            "url": "https://yimingli.net/differential-privacy-for-social-scientists.html",
            "relUrl": "/differential-privacy-for-social-scientists.html",
            "date": " • Aug 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "COVID-19: Your State is Not That Different",
            "content": "This post is inspired by the following work: . The smoothed log-log plot of cumulative cases against new cases, by Aatish Bhatia and Minute Physics. The log-log scale and the stock vs (smoothed 7-day) flow help us to see the exponential path each country was on, and make it easier to spot when the growth has passed the exponential growth phase and started to slow down. | Kieran Healy&#39;s replication of John Burn-Murdoch&#39;s small-multiple plot of confirmed cases by country. The technique is useful to contrast a country&#39;s trajectory with other countries trajectories, putting things into perspective. | . I combine these two approaches to show that all 50 States and Washington DC in the United States saw exponential growth in confirmed cases, and have not shown clear signs of slowing down, with some caveats below. . A few notes to help with the interpretation of the graph: . X axis is the cumulative number of confirmed cases in log10 scale, and the y axis is the new confirmed cases in the last 7 days in log10 scale. | Each day corresponds to a point on each of the graphs. Imaging a dot climbing up to the top right corner of each graph, with the large red dot representing the latest day, leaving a trace behind it. | The gray background lines of each small-multiple panel are the growth paths of all other states. | The 7-day period in the y axis is to smooth the daily fluctuations in new confirmed cases. | If the growth of confirmed cases are exponential, the slope of log(confirmed cases) vs log(total new cases in the last 7 days) is 45 degrees. Therefore the fact that every state is on the 45-degree line suggests every state is on an exponential growth path. | But the 45-degree line does not suggest all states share the same rate of growth, just that all states are on some exponential path. | This graph only considers confirmed/detected cases, not the actually infected cases, which are only larger than detected cases. Limited testing is likely an important factor that confounds the interpretation: the actual spread of the virus could be faster or slower than the increase in confirmed cases. Combining confirmed cases and hospitalizations trends may help paint a clearer picture. Although a shorter time series, the hospitalization trends are also exponential in states that report the data. | The chart is not intended to be predictive: it&#39;s tracking the smoothed trends to see where we are, not where we are going to be. Specifically, it&#39;s tracking whether or not the confirmed cases are still on an exponential growth trajectory. | .",
            "url": "https://yimingli.net/us-covid-trends-by-state.html",
            "relUrl": "/us-covid-trends-by-state.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Notes: McCloskey - 1998 - the So-called Coase Theorem",
            "content": "McCloskey, Deirdre. “The so-called Coase theorem.” Eastern Economic Journal 24, no. 3 (1998): 367-371. . George Stigler’s version of the Coase Theorem, in the 1960s It doesn’t matter where you place the liability for, say, smoke pollution, because in a world of zero transaction costs, the right to pollute will end up in the hands that value it the most. | In a textbook of Shughart, Chappell, and Cottle: “In the absence of transaction costs, the allocation of resources is independent of the initial assignment of property rights.” | . | Stigler’s theorem is actually Adam Smith’s theorem (1776). It is wholly explicit in Edgeworth (1881, 30ff, 114); and with all the bells and whistles in Arrow and Debreu (1954). Smith, Edgeworth, Arrow, Debreu, with many others, noted that an item gravitates by exchange into the hands of the person who values it the most, if transaction cost (such as transportation) are not too high. | . | Coase’s actual point was to note what happens in the many important cases in which transactions costs cannot be neglected. If the situation does have high transactions costs, then it does matter where the liability for pollution is placed. In consequence, the economist’s preference for a simple, blackboard solution, taxing the party that “causes” the pollution (as Pigou and Samuelson suggest), is no longer defensible. | McCloskey 1993, footnote 2 Coase’s article was not meant to show that we live already in the best of all possible worlds (as Stigler was inclined to assume in this and other cases) but on the contrary that if we did there would of course be no need for policy; and that in fact, as Coase argued also in the 1937 article, transaction costs push our world unpredictably far from the blackboard optimum [thus second best]. | . | Email conversation between McCloskey and Weisskopf McCloskey: In the presence of transaction costs, one cannot in general efficiently internalize an externality by taxing/subsidizing whoever is generating the negative/positive externalities, because it would generally not result in the right to the resource affected going to the person who values it the most. | McCloskey: In other words, whoever is generating the negative/positive externalities is not a meaningful notion. In the railway/farmers example: who “causes” the burnt of fields of corn, the railway which makes the sparks, or the farmers who plant imprudently close to the line? | In the noise pollution around airports example: are the airplanes the “cause” of the pollution, or the ears that near the airports? The presence of ears is just as much a “cause” as the vibrations in the planes’ motors. Where then should the Pigou/Samuelson tax be placed? | . | Weisskopf: Is it true that Coase implies not that state should get out of the business of dealing with externalities, but implies that states should try to get the entitlements right rather than getting the prices right? | McCloskey: Yes. But Coase also claims, with considerable empirical evidence, that in many cases, government trying to get the entitlements right is worse than laissez faire (FCC or the lighthouses or the law of liability). In most cases, because of lack of knowledge. | McCloskey: In other words, Coase claims that laissez faire is desirable not because it’s the First Best (transaction costs are low, things will come into places), but because it is the Second Best (the transaction costs are so high that entitlements matters, and transaction costs are so high that it is hard for the government to get the entitlements right). | McCloskey: It is this claim, that lead people to confuse Coase as yet another laissez faire economist. | . | In a wider context, Pigou, Samuelson, and Stiger are “modernists”. Modernism has had some good moments, such as the Ronchamp Chapel by Le Corbusier or The Foundations of Economic Analysis by Paul Samuelson. It was worth a try, though on the whole it did not work very well. Have a look at economic policy; or, if that doesn’t appall you, then look at the average academic article in economics, pure theoretical modernism, blackboard economics gone loco. | I commend Coase for his old-fashioned ways. The old-fashioned ways have become the latest fashion, actually. You read it here: Ronald Coase is a postmodern economist. And his theorem, a post-modern one, is about the difficulty of knowing what is to be done. | .",
            "url": "https://yimingli.net/notes-mccloskey-on-coase-theorem.html",
            "relUrl": "/notes-mccloskey-on-coase-theorem.html",
            "date": " • Jun 18, 2013"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I am a Research Scientist working on privacy-preserving technologies, causal inference, and experimentation platforms at Facebook. . I hold a Ph.D. in Public Policy from the University of Chicago, where I studied Labor Economics, Public Economics, and Applied Microeconometrics. This website is powered by fastpages. .",
          "url": "https://yimingli.net/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yimingli.net/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}