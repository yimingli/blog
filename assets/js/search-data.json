{
  
    
        "post0": {
            "title": "A Monte Carlo Simulation Workflow in Python",
            "content": "Introduction . This is a workflow for Monte Carlo Simulation in Python, using a dataframe to track the parameters, simulated dataset, and summary statistics / model fitting. For multi-core machines, we can try to speed up using parallel computation. . This workflow is inspired by R&#39;s purrr package and the notion of list-columns. I also benefited from Alex Hayes&#39; great note on many models workflow in python. . Running example in this note: . Simulate many of datasets from each of the 9 normal distribution, with 3 different centers and 3 different scales. | Calculate the sample statistics for each of the simulated datasets. | Put everything in a dataframe. | The workflow consists of 4 steps: . Set the model parameters | Simulate datasets according to the parameters | Do stuff with the simulated datasets, e.g., calculate sample statistics, fit models. | Collect steps 1--3 in a dataframe. The main benefit of keeping everything in a dataframe is for later analysis and visualization of the Monte Carlo simulation. | from platform import python_version print(python_version()) . 3.7.8 . import concurrent.futures from itertools import product from typing import Dict, List, Union import numpy as np import pandas as pd . import psutil n_cores = psutil.cpu_count() n_cores . 16 . Set model parameters . N_REP = 1000 # sample size can also be a parameter of simulation, but set to 1 value for illustration SAMPLE_SIZE = [10_000] locations = [0, 1, 2] scales = [1, 2, 3] params = pd.DataFrame( list(product(locations, scales, range(N_REP), SAMPLE_SIZE)), columns=[&quot;loc&quot;, &quot;scale&quot;, &quot;rep&quot;, &quot;size&quot;], ) params . loc scale rep size . 0 0 | 1 | 0 | 10000 | . 1 0 | 1 | 1 | 10000 | . 2 0 | 1 | 2 | 10000 | . 3 0 | 1 | 3 | 10000 | . 4 0 | 1 | 4 | 10000 | . ... ... | ... | ... | ... | . 8995 2 | 3 | 995 | 10000 | . 8996 2 | 3 | 996 | 10000 | . 8997 2 | 3 | 997 | 10000 | . 8998 2 | 3 | 998 | 10000 | . 8999 2 | 3 | 999 | 10000 | . 9000 rows × 4 columns . Simulate datasets . For small dataset, we can keep the simulated data sets in the tracking dataframe. If the simulated dataset is large, we only keep the sample statistics in the dataframe. . def simulate_data_seq(params: pd.DataFrame) -&gt; pd.DataFrame: rng = np.random.default_rng() sim_data = pd.DataFrame() sim_data[&quot;sample&quot;] = [ rng.normal(loc=loc, scale=scale, size=size) for loc, scale, size in zip(params[&quot;loc&quot;], params[&quot;scale&quot;], params[&quot;size&quot;]) ] return sim_data . %%time sim_data_seq = simulate_data_seq(params) . CPU times: user 1.34 s, sys: 177 ms, total: 1.51 s Wall time: 1.52 s . Do stuff with the simulated datasets . def calculate_sample_stats(arr: np.ndarray) -&gt; Dict: &quot;&quot;&quot;calculate sample statistics&quot;&quot;&quot; average = arr.mean() variance = arr.var(ddof=1) p05 = np.quantile(arr, 0.05) p95 = np.quantile(arr, 0.95) return { &quot;average&quot;: average, &quot;variance&quot;: variance, &quot;p05&quot;: p05, &quot;p95&quot;: p95, } def compute_seq(df: pd.DataFrame) -&gt; pd.DataFrame: sample_stats = [calculate_sample_stats(sample) for sample in df[&quot;sample&quot;]] return pd.DataFrame.from_records(sample_stats) . %%time sample_stats_seq = compute_seq(sim_data_seq) . CPU times: user 3.32 s, sys: 12.3 ms, total: 3.34 s Wall time: 3.34 s . Keep everything in a dataframe . pd.concat([params, sim_data_seq, sample_stats_seq], axis=&quot;columns&quot;) . loc scale rep size sample average variance p05 p95 . 0 0 | 1 | 0 | 10000 | [0.319077282511964, 0.18965678964333085, -0.63... | -0.018138 | 1.000294 | -1.665284 | 1.624925 | . 1 0 | 1 | 1 | 10000 | [1.0530038923608775, 0.6534644477062702, 1.385... | -0.006782 | 1.008830 | -1.632020 | 1.693900 | . 2 0 | 1 | 2 | 10000 | [0.619682511046317, 0.8076221364639493, -0.212... | -0.003659 | 0.977135 | -1.634407 | 1.627604 | . 3 0 | 1 | 3 | 10000 | [-0.7515129763643353, -0.8428228923508118, -0.... | 0.007276 | 0.994398 | -1.652222 | 1.654156 | . 4 0 | 1 | 4 | 10000 | [-0.3187877346174399, -0.09063484844832305, -0... | 0.001544 | 1.005466 | -1.639467 | 1.663695 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 8995 2 | 3 | 995 | 10000 | [4.459473155059797, 7.17871505746818, 2.080059... | 1.981215 | 9.112403 | -3.050421 | 6.898461 | . 8996 2 | 3 | 996 | 10000 | [1.3252498963188968, -7.932355926994303, 5.374... | 1.984005 | 8.874890 | -3.008920 | 6.929186 | . 8997 2 | 3 | 997 | 10000 | [-5.566903275579494, 4.783448759876334, -1.454... | 1.999839 | 8.816550 | -2.910886 | 6.922024 | . 8998 2 | 3 | 998 | 10000 | [4.783361765266323, 0.06270761044701434, 6.484... | 2.014064 | 8.844529 | -2.876956 | 6.945541 | . 8999 2 | 3 | 999 | 10000 | [3.597228274125008, 1.7495518614255168, 6.9812... | 1.958327 | 8.951227 | -2.915028 | 6.874206 | . 9000 rows × 9 columns . Parallel computation . We may be able to speed up the simulation and computation using parallel computation, e.g., multiprocessing, multithreading, or other packages, depending on whether the bottlenecks are the I/O, memory, cache, or CPU. . Below is an example of multiprocessing. A few notes when using multiprocessing in Python. . Depending on the size of the datasets, number of datasets, and how cpu-hungry the computations are, multiprocessing may not perform better than sequential processing. In my working example, multiprocessing did not improve performance. In my real simulation work with heavier computations, multiprocessing does help. | We need to change the simulation function a little in terms how it takes in arguments, to facilitate the map function. | Chunksize may be a factor for performance. | Numpy, Scipy, and Pandas scientific computing packages may not play well with multiprocessing. Read more here. | . CHUNKSIZE = int(params.shape[0] / (n_cores / 2)) CHUNKSIZE . 1125 . def simulate_data(params: Dict) -&gt; np.ndarray: loc = params[&quot;loc&quot;] # float scale = params[&quot;scale&quot;] # float size = params[&quot;size&quot;] # int # be sure to reseed the RNG # see discussions here: https://github.com/numpy/numpy/issues/9650 rng = np.random.default_rng() return rng.normal(loc=loc, scale=scale, size=size) def simulate_data_mp(params: List) -&gt; Dict: params_list = params.to_dict(orient=&quot;records&quot;) sim_data = pd.DataFrame() with concurrent.futures.ProcessPoolExecutor() as executor: sim_data[&quot;sample&quot;] = list( executor.map(simulate_data, params_list, chunksize=CHUNKSIZE) ) return sim_data . %%time sim_data_mp = simulate_data_mp(params) . CPU times: user 619 ms, sys: 4.15 s, total: 4.77 s Wall time: 6.22 s . def compute_mp(df: pd.DataFrame) -&gt; pd.DataFrame: with concurrent.futures.ProcessPoolExecutor() as executor: sample_stats = list( executor.map(calculate_sample_stats, df[&quot;sample&quot;], chunksize=CHUNKSIZE) ) return pd.DataFrame.from_records(sample_stats) . %%time sample_stats_mp = compute_mp(sim_data_seq) . CPU times: user 403 ms, sys: 683 ms, total: 1.09 s Wall time: 5.13 s .",
            "url": "https://yimingli.net/a-monte-carlo-simulation-workflow-in-python.html",
            "relUrl": "/a-monte-carlo-simulation-workflow-in-python.html",
            "date": " • Nov 27, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I am a Research Scientist working on privacy-preserving technologies, causal inference, and experimentation platforms at Facebook. . I hold a Ph.D. in Public Policy from the University of Chicago, where I studied Labor Economics, Public Economics, and Applied Microeconometrics. This website is powered by fastpages. .",
          "url": "https://yimingli.net/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yimingli.net/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}