{
  
    
        "post0": {
            "title": "A Monte Carlo Simulation Workflow in Python",
            "content": "Introduction . This is a workflow for Monte Carlo Simulation in Python, using a dataframe to track the parameters, simulated dataset, and summary statistics / model fitting. For multi-core machines, we can try to speed up using parallel computation. . This workflow is inspired by R&#39;s purrr package and the notion of list-columns. I also benefited from Alex Hayes&#39; great note on many models workflow in python. . Running example in this note: . Simulate many of datasets from each of the 9 normal distribution, with 3 different centers and 3 different scales. | Calculate the sample statistics for each of the simulated datasets. | Put everything in a dataframe. | The workflow consists of 4 steps: . Set the model parameters | Simulate datasets according to the parameters | Do stuff with the simulated datasets, e.g., calculate sample statistics, fit models. | Collect steps 1--3 in a dataframe. The main benefit of keeping everything in a dataframe is for later analysis and visualization of the Monte Carlo simulation. | from platform import python_version print(python_version()) . 3.7.8 . import concurrent.futures from itertools import product from typing import Dict, List, Union import numpy as np import pandas as pd . import psutil n_cores = psutil.cpu_count() n_cores . 16 . Set model parameters . N_REP = 1000 # sample size can also be a parameter of simulation, but set to 1 value for illustration SAMPLE_SIZE = [10_000] locations = [0, 1, 2] scales = [1, 2, 3] params = pd.DataFrame( list(product(locations, scales, range(N_REP), SAMPLE_SIZE)), columns=[&quot;loc&quot;, &quot;scale&quot;, &quot;rep&quot;, &quot;size&quot;], ) params . loc scale rep size . 0 0 | 1 | 0 | 10000 | . 1 0 | 1 | 1 | 10000 | . 2 0 | 1 | 2 | 10000 | . 3 0 | 1 | 3 | 10000 | . 4 0 | 1 | 4 | 10000 | . ... ... | ... | ... | ... | . 8995 2 | 3 | 995 | 10000 | . 8996 2 | 3 | 996 | 10000 | . 8997 2 | 3 | 997 | 10000 | . 8998 2 | 3 | 998 | 10000 | . 8999 2 | 3 | 999 | 10000 | . 9000 rows × 4 columns . Simulate datasets . For small dataset, we can keep the simulated data sets in the tracking dataframe. If the simulated dataset is large, we only keep the sample statistics in the dataframe. . def simulate_data_seq(params: pd.DataFrame) -&gt; pd.DataFrame: rng = np.random.default_rng() sim_data = pd.DataFrame() sim_data[&quot;sample&quot;] = [ rng.normal(loc=loc, scale=scale, size=size) for loc, scale, size in zip(params[&quot;loc&quot;], params[&quot;scale&quot;], params[&quot;size&quot;]) ] return sim_data . %%time sim_data_seq = simulate_data_seq(params) . CPU times: user 1.34 s, sys: 177 ms, total: 1.51 s Wall time: 1.52 s . Do stuff with the simulated datasets . def calculate_sample_stats(arr: np.ndarray) -&gt; Dict: &quot;&quot;&quot;calculate sample statistics&quot;&quot;&quot; average = arr.mean() variance = arr.var(ddof=1) p05 = np.quantile(arr, 0.05) p95 = np.quantile(arr, 0.95) return { &quot;average&quot;: average, &quot;variance&quot;: variance, &quot;p05&quot;: p05, &quot;p95&quot;: p95, } def compute_seq(df: pd.DataFrame) -&gt; pd.DataFrame: sample_stats = [calculate_sample_stats(sample) for sample in df[&quot;sample&quot;]] return pd.DataFrame.from_records(sample_stats) . %%time sample_stats_seq = compute_seq(sim_data_seq) . CPU times: user 3.32 s, sys: 12.3 ms, total: 3.34 s Wall time: 3.34 s . Keep everything in a dataframe . pd.concat([params, sim_data_seq, sample_stats_seq], axis=&quot;columns&quot;) . loc scale rep size sample average variance p05 p95 . 0 0 | 1 | 0 | 10000 | [0.319077282511964, 0.18965678964333085, -0.63... | -0.018138 | 1.000294 | -1.665284 | 1.624925 | . 1 0 | 1 | 1 | 10000 | [1.0530038923608775, 0.6534644477062702, 1.385... | -0.006782 | 1.008830 | -1.632020 | 1.693900 | . 2 0 | 1 | 2 | 10000 | [0.619682511046317, 0.8076221364639493, -0.212... | -0.003659 | 0.977135 | -1.634407 | 1.627604 | . 3 0 | 1 | 3 | 10000 | [-0.7515129763643353, -0.8428228923508118, -0.... | 0.007276 | 0.994398 | -1.652222 | 1.654156 | . 4 0 | 1 | 4 | 10000 | [-0.3187877346174399, -0.09063484844832305, -0... | 0.001544 | 1.005466 | -1.639467 | 1.663695 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 8995 2 | 3 | 995 | 10000 | [4.459473155059797, 7.17871505746818, 2.080059... | 1.981215 | 9.112403 | -3.050421 | 6.898461 | . 8996 2 | 3 | 996 | 10000 | [1.3252498963188968, -7.932355926994303, 5.374... | 1.984005 | 8.874890 | -3.008920 | 6.929186 | . 8997 2 | 3 | 997 | 10000 | [-5.566903275579494, 4.783448759876334, -1.454... | 1.999839 | 8.816550 | -2.910886 | 6.922024 | . 8998 2 | 3 | 998 | 10000 | [4.783361765266323, 0.06270761044701434, 6.484... | 2.014064 | 8.844529 | -2.876956 | 6.945541 | . 8999 2 | 3 | 999 | 10000 | [3.597228274125008, 1.7495518614255168, 6.9812... | 1.958327 | 8.951227 | -2.915028 | 6.874206 | . 9000 rows × 9 columns . Parallel computation . We may be able to speed up the simulation and computation using parallel computation, e.g., multiprocessing, multithreading, or other packages, depending on whether the bottlenecks are the I/O, memory, cache, or CPU. . Below is an example of multiprocessing. A few notes when using multiprocessing in Python. . Depending on the size of the datasets, number of datasets, and how cpu-hungry the computations are, multiprocessing may not perform better than sequential processing. In my working example, multiprocessing did not improve performance. In my real simulation work with heavier computations, multiprocessing does help. | We need to change the simulation function a little in terms how it takes in arguments, to facilitate the map function. | Chunksize may be a factor for performance. | Numpy, Scipy, and Pandas scientific computing packages may not play well with multiprocessing. Read more here. | . CHUNKSIZE = int(params.shape[0] / (n_cores / 2)) CHUNKSIZE . 1125 . def simulate_data(params: Dict) -&gt; np.ndarray: loc = params[&quot;loc&quot;] # float scale = params[&quot;scale&quot;] # float size = params[&quot;size&quot;] # int # be sure to reseed the RNG # see discussions here: https://github.com/numpy/numpy/issues/9650 rng = np.random.default_rng() return rng.normal(loc=loc, scale=scale, size=size) def simulate_data_mp(params: List) -&gt; Dict: params_list = params.to_dict(orient=&quot;records&quot;) sim_data = pd.DataFrame() with concurrent.futures.ProcessPoolExecutor() as executor: sim_data[&quot;sample&quot;] = list( executor.map(simulate_data, params_list, chunksize=CHUNKSIZE) ) return sim_data . %%time sim_data_mp = simulate_data_mp(params) . CPU times: user 619 ms, sys: 4.15 s, total: 4.77 s Wall time: 6.22 s . def compute_mp(df: pd.DataFrame) -&gt; pd.DataFrame: with concurrent.futures.ProcessPoolExecutor() as executor: sample_stats = list( executor.map(calculate_sample_stats, df[&quot;sample&quot;], chunksize=CHUNKSIZE) ) return pd.DataFrame.from_records(sample_stats) . %%time sample_stats_mp = compute_mp(sim_data_seq) . CPU times: user 403 ms, sys: 683 ms, total: 1.09 s Wall time: 5.13 s .",
            "url": "https://yimingli.net/a-monte-carlo-simulation-workflow-in-python.html",
            "relUrl": "/a-monte-carlo-simulation-workflow-in-python.html",
            "date": " • Nov 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Differential Privacy for Social Scientists",
            "content": "This year’s NBER (National Bureau of Economic Research) Summer Institute Methods Lecture is on Differential Privacy. . Sessions: . Differential Privacy: Observations for Economists – Daniel Goroff: video, slides | Introduction to Differential Privacy – Daniel Kifer: video, slides | Decisions with Privacy-Protected Data – Ian Schmutte: video, slides | Basic Differential Privacy Algorithms and Statistics – Daniel Kifer: video, slides | Formal Privacy in Census Data – Ian Schmutte: video, slides | Implications of Data Privacy Concerns for Empirical Social Science – Frauke Kreuter: video, paper | .",
            "url": "https://yimingli.net/differential-privacy-for-social-scientists.html",
            "relUrl": "/differential-privacy-for-social-scientists.html",
            "date": " • Aug 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "COVID-19: Your State is Not That Different",
            "content": "This post is inspired by the following work: . The smoothed log-log plot of cumulative cases against new cases, by Aatish Bhatia and Minute Physics. The log-log scale and the stock vs (smoothed 7-day) flow help us to see the exponential path each country was on, and make it easier to spot when the growth has passed the exponential growth phase and started to slow down. | Kieran Healy&#39;s replication of John Burn-Murdoch&#39;s small-multiple plot of confirmed cases by country. The technique is useful to contrast a country&#39;s trajectory with other countries trajectories, putting things into perspective. | . I combine these two approaches to show that all 50 States and Washington DC in the United States saw exponential growth in confirmed cases, and have not shown clear signs of slowing down, with some caveats below. . A few notes to help with the interpretation of the graph: . X axis is the cumulative number of confirmed cases in log10 scale, and the y axis is the new confirmed cases in the last 7 days in log10 scale. | Each day corresponds to a point on each of the graphs. Imaging a dot climbing up to the top right corner of each graph, with the large red dot representing the latest day, leaving a trace behind it. | The gray background lines of each small-multiple panel are the growth paths of all other states. | The 7-day period in the y axis is to smooth the daily fluctuations in new confirmed cases. | If the growth of confirmed cases are exponential, the slope of log(confirmed cases) vs log(total new cases in the last 7 days) is 45 degrees. Therefore the fact that every state is on the 45-degree line suggests every state is on an exponential growth path. | But the 45-degree line does not suggest all states share the same rate of growth, just that all states are on some exponential path. | This graph only considers confirmed/detected cases, not the actually infected cases, which are only larger than detected cases. Limited testing is likely an important factor that confounds the interpretation: the actual spread of the virus could be faster or slower than the increase in confirmed cases. Combining confirmed cases and hospitalizations trends may help paint a clearer picture. Although a shorter time series, the hospitalization trends are also exponential in states that report the data. | The chart is not intended to be predictive: it&#39;s tracking the smoothed trends to see where we are, not where we are going to be. Specifically, it&#39;s tracking whether or not the confirmed cases are still on an exponential growth trajectory. | .",
            "url": "https://yimingli.net/us-covid-trends-by-state.html",
            "relUrl": "/us-covid-trends-by-state.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "控制变量是如何被"控制"的？",
            "content": "注：下文是我在知乎的回答。原链接：https://www.zhihu.com/question/326199739/answer/710278097 . 题主似乎问了两个问题。 . 回归中，自变量 X 和控制变量 Z 的地位应该是平等/对称的，为什么解释起来不一样？ | 回归系数的解释总是说「保持 Z 不变，1 个单位的 X 增加和 betaX 个单位的 y 增加相关联」，为什么回归系数能保持 Z 不变，原理是什么？ | 第一个问题，自变量 X 和控制变量 Z 在 OLS 算法的眼里确实是一样的，所以回归系数的机械解释也是一样的。假设我们的线性回归模型是 y = beta0 + betaX * X + betaZ * Z + u，其中 y 是年工资 (元），X 是上学年限（年），Z 是母亲上学年限（年）。betaX 和 betaZ 分别是 X 和 Z 的回归系数。 . betaX 的解释是「如果母亲上学年限一样，平均来讲，多上一年学的小明比少上一年学的小红每年多赚 betaX 元」。 | betaZ 的解释是「如果小明和小红的上学年限一样，但小明的妈妈比小红的妈妈多上一年学，平均来讲，小明的年工资比小红多 betaZ 元」。 | . 在这个意义上，「自变量」和「控制变量」只不过是「感兴趣变量」和「不怎么感兴趣变量」的不同标签。 . 但上面的解释只是相关性，一旦涉及到因果解释，就要用到现实世界的因果模型，betaX 和 betaZ 的解释就不一样了。假设我们确信真实的因果模型如下图所示： . . 自身的上学年限会影响自身的工资 | 母亲的上学年限会影响子女的上学年限，也会以除了子女教育以外的其他方式影响子女的收入（比如受教育程度高的母亲可能会更有意识的培养子女的软技能，从而影响子女工资） | 祖母的上学年限会影响母亲的上学年限，也会以其他方式影响孙子孙女的工资（比如受教育程度高的祖母可能有更广的人脉，从而影响孙子孙女的工作和工资） | . 在这个因果模型下，咱们的回归模型 y = beta0 + betaX * X + betaZ * Z + u 中 betaX 是因果解释：给定母亲的上学年限相同，平均来讲，每多上一年学能增加工资 betaX。这是因为加了母亲上学年限 Z 之后，X 和 u 就不相关了, E (Xu | Z) = 0，所以 betaX 是无偏的。而 betaZ 就只能是相关解释：我们不能说母亲每多上一年学，就能增加子女的工资 betaZ，只能说母亲的教育水平和子女的工资是正相关的（conditional on 子女的受教育水平），这是因为祖母上学年限在 u 里面，所以 Z 与 u 是相关的，E(Zu | X) != 0，betaZ 是有偏的。 . 许多研究中，希望得到因果解释的那个变量就是「自变量」，而其他变量只是帮助自变量得到因果解释。所以控制变量有时候也被叫做 nuisance variable. . 第二个问题，回归系数的解释总是说「保持 Z 不变，1 个单位的 X 增加和 betaX 个单位的 y 增加相关联」，为什么回归系数能保持 Z 不变，原理是什么？ . 这个「保持 Z 不变」或者「控制 Z 在相同水平」的说法是从早期用回归分析实验的时候遗留下来的，适用于实验数据，并不适合观察数据。因为在实验中，你确实可以控制一个变量在不同个体之间不变，在观察数据中做不到。用观察数据做回归得出的回归系数，是平均上面再叠加平均，强行用「保持 Z 不变」来解释结果就很勉强。所以 Andrew Gelman 就建议不要用 control for Z，而是 (attempt to) adjust for Z. 「控制变量」(control variable) 叫做 「协变量」(covariate) 可能更恰当。 . 至于回归是怎么 adjust for Z 的，参考其他几个答案。另外，Frisch–Waugh–Lovell theorem 了解一下，对理解最小方差回归的原理有帮助。沿用上面的例子简单说，就是把工资和自身上学年限中受到母亲教育影响的部分剔除，用剩下的部分做简单的相关分析。这个时候工资和上学年限的关系就不再受到母亲教育程度的影响，在这个意义上母亲教育程度的影响被「控制」住了。 .",
            "url": "https://yimingli.net/what-are-we-talking-about-when-we-talk-about-control-variables.html",
            "relUrl": "/what-are-we-talking-about-when-we-talk-about-control-variables.html",
            "date": " • Jun 9, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "劳动经济学领域有哪些前沿的研究",
            "content": "注：下文是我在知乎的回答。原链接：https://www.zhihu.com/question/61126343/answer/269808062 . 每个国家的经济学家关注的问题都有所不同，我提几个在美国比较受关注的劳动经济学实证领域。 . 职业执照 . 在美国，职业执照渐渐成为政府对劳动市场管制的主要形式之一。目前 1/4 到 1/3 的美国劳动人口需要各级政府颁发的执照才能合法工作，这个数字在 1970 年代大约是 1/10 . 越来越多的劳动市场准入管制对劳动供给、工资、职业技能构成、职业素质、服务质量、职业流动、跨州迁徙、收入不均、前犯罪人口再就业等等的影响。 . 专业/职业选择 . 一方面是代际之间职业选择的流动性。一方面是不同群体的职业选择差别，比如男性 vs 女性，年轻人 vs baby boomers。职业选择又跟职业执照、职场歧视、工作时长和灵活度等有着种种联系。 . 国内移民 . 美国州际移民在最近的几十年稳步下降。衰落的地区民众没法转移到生产力高的地区。这又跟劳动市场、技能培训、zoning laws、国外移民、总统选举啥的息息相关。Chetty 团队对儿童成长的地理位置的研究，挑战了对 Moving to Opportunity 的早期研究。可以关注 Abigail Wozniak 近年的研究。 . 这个话题也是中国当前的热点。人家是担心大家为啥不往城里搬了，咱们是琢磨着怎么把人赶出去。 . 早期（学前）教育 . 知乎介绍应该有不少。读文献的话从 Jim Heckman 那边顺藤摸瓜就是了。 . 性别差异 . 两性差异在劳动市场各个维度的实证证据近几年积累很快：考试成绩、入学率和毕业率、大学专业选择、工资报酬、工作时长、工作灵活度、职业选择、职业发展、生育选择。这个话题 @Manolo 有许多介绍，建议关注。 . 失业 . 失业当然是劳动经济学的长期话题。几年有不少研究关注经济危机之后劳动市场和相应政府救助项目的变化。也有一些学者开始关注数字经济对年轻人的劳动供给的影响。比如我老板最近写了一篇论文说电子游戏做的越来越好，搞得年轻人们都不愿意工作了。。。 . 家庭 . 大部分传统劳动经济学研究把家庭 (household) 当成一个黑箱，并不关注家庭内部发生了什么。Becker 之后一部分学者慢慢试着打开这个黑箱。家庭内部的如何做出一些重要决策：生育决策、父母和子女的教育决策、婚嫁决策、劳动供给决策、配偶间的 power dynamic 及其决定因素。这个研究圈子似乎比较小，推荐关注本校和蔼可亲的新星 Alessandra Voena 教授。 .",
            "url": "https://yimingli.net/labor-economics-topics.html",
            "relUrl": "/labor-economics-topics.html",
            "date": " • Dec 2, 2017"
        }
        
    
  
    
        ,"post5": {
            "title": "禁毒政策的成本和中间选项",
            "content": "注：下文是我在知乎的回答。原问题是 “支持大麻合法化的人是什么想法？” https://www.zhihu.com/question/24919994/answer/136924382 . 我在两个方面做一些补充。第一是禁毒政策的成本，第二是禁毒和完全自由放任之间的其它选项。另外，只谈美国的情况。 . 禁毒政策的成本 . 吸毒有负的外部性，禁毒同样有副作用。目前大部分回答完全没有提及当前禁毒政策的成本。100 条禁毒的好处，也可能小于禁毒政策的副作用。逻辑一致的支持者或反对者都应该考虑禁毒政策的成本。以下列举一些全面禁毒的副作用： . 2010 年美国有超过 160 万人次因为毒品被捕，其中超过 80% 的人是因为”拥有毒品”，而不是吸食、销售、生产毒品。其中超过 75 万人是因为拥有大麻被捕。这仅仅是一年的人数。许多被捕的人即使没有定罪，也会因此丢工作。而且被捕记录在美国是公开信息，会持续影响被捕人的整个职业生涯。 | 据 Gary Becker 和 Kevin Murphy 的估算，美国在禁毒方面的显性支出约为每年500 亿美元，包括执法、司法、监狱。 | 禁毒的隐形成本可能比显性成本高得多，但是很难量化。比如： | 禁毒导致的高价格和高利润催生了大量暴力。美国大中城市的暴力犯罪率尤其是谋杀率居高不下的原因之一就是与毒品相关的黑帮暴力。 | 禁毒导致的高利润把禁毒战争的战火烧到周边国家。墨西哥从 06 年开始到 16 年，约 15 万人死于毒品相关的暴力，2 万 8 千人失踪。哥伦比亚、危地马拉、洪都拉斯等南美国家的暴力都与美国的禁毒政策密切关联。 | 执法中死亡、受伤的执法人员。 | 多数美国大中城市都有毒品暴力造成的贫民窟式的社区。这些社区不光暴力盛行，孩子也无法安心上学，许多社区的高中辍学率持续在 25% 以上，辍学以后基本找不到工作，转而为毒贩卖命，造成恶性循环。 | 巨大的毒品利益带来的政府腐败。 | 禁毒政策提高了软毒品的相对价格，许多软毒品使用者转向硬毒品。 | . 以上禁毒带来的负面后果都是事前可以预期的。这些后果在美国禁酒令期间就已经清清楚楚的发生过，而且在全面禁毒的半个多世纪也积累了许多证据。所以一个逻辑一致的禁毒支持者在支持禁毒的同时，也应该证明禁毒的正面效果超过这些负面效果的总和。你可以声泪俱下用成瘾者家破人亡的例子来控诉毒品的危害，别人同样可以声泪俱下用毒品暴力受害者家破人亡的例子来反驳你。 . 介于完全放开和完全禁止之间的可能性 . 目前大多数回答认为管制毒品只有两种手段：完全禁止，或者完全放开。许多禁毒支持者考虑到吸毒的潜在成瘾性，吸毒可能带来的负外部性，和未成年人吸毒等问题，很自然的觉得毒品当然不可以完全放开，于是支持完全禁止。但介于两者之间其实有许多可行的且副作用少得多的方案。我简单介绍几种。主要来自芝加哥大学 Jim Leitzel 的几篇论文。 . 酒精、烟草、赌博都可能让人上瘾，都有负外部性。从对酒精、烟草、赌博的管制中，我们或许可以借鉴一些经验。以下列举一些常见的管制手段： . 年龄限制 | 卖家资质限制 | 买家资质限制 | 消费场合限制；公共场合 vs 私人场合；店内消费 vs 外带消费等等 | 特种税 | 安全使用知识考试 | . 这里对买家资质限制展开讲一下。方案一：初始的买家资质根据年龄等因素默认发放。但初始资质可以收回：如果发现滥用，转手，使用后暴力，使用后驾车，在非法场合使用等等，买家资质将被收回。方案二：初始的买家资质不是默认发放，而是默认需要申请。 . 还可以加入在赌博业中常见的自愿退出机制。比如可以自愿在未来 5 年甚至终生不购买。还可以允许第三方申请强制退出，比如家庭成员可以在特定情况下干预，这在赌博业中也不鲜见。另外，买家资质还可以根据行为学的研究加入其他一些默认选项。比如默认资质只能购买“软毒品”，默认的剂量可以很低，默认的可购买频率可以很低。可以允许合格的买家改变默认选项，但有必要的程序和限制。 . 不同烈度的毒品可以根据监控难度、实际效果等采取不同的方案。 . 最后强调一下，各国的情况不同，政策成本和可选项都不同，需要仔细研究。但在讨论是否应该维持全面禁毒政策的时候，不要忽略禁毒政策的副作用，也不要忽略除了全面禁止和完全开放之外的政策选项。 .",
            "url": "https://yimingli.net/on-drug-policies.html",
            "relUrl": "/on-drug-policies.html",
            "date": " • Dec 21, 2016"
        }
        
    
  
    
        ,"post6": {
            "title": "Notes: McCloskey - 1998 - the So-called Coase Theorem",
            "content": "McCloskey, Deirdre. “The so-called Coase theorem.” Eastern Economic Journal 24, no. 3 (1998): 367-371. . George Stigler’s version of the Coase Theorem, in the 1960s It doesn’t matter where you place the liability for, say, smoke pollution, because in a world of zero transaction costs, the right to pollute will end up in the hands that value it the most. | In a textbook of Shughart, Chappell, and Cottle: “In the absence of transaction costs, the allocation of resources is independent of the initial assignment of property rights.” | . | Stigler’s theorem is actually Adam Smith’s theorem (1776). It is wholly explicit in Edgeworth (1881, 30ff, 114); and with all the bells and whistles in Arrow and Debreu (1954). Smith, Edgeworth, Arrow, Debreu, with many others, noted that an item gravitates by exchange into the hands of the person who values it the most, if transaction cost (such as transportation) are not too high. | . | Coase’s actual point was to note what happens in the many important cases in which transactions costs cannot be neglected. If the situation does have high transactions costs, then it does matter where the liability for pollution is placed. In consequence, the economist’s preference for a simple, blackboard solution, taxing the party that “causes” the pollution (as Pigou and Samuelson suggest), is no longer defensible. | McCloskey 1993, footnote 2 Coase’s article was not meant to show that we live already in the best of all possible worlds (as Stigler was inclined to assume in this and other cases) but on the contrary that if we did there would of course be no need for policy; and that in fact, as Coase argued also in the 1937 article, transaction costs push our world unpredictably far from the blackboard optimum [thus second best]. | . | Email conversation between McCloskey and Weisskopf McCloskey: In the presence of transaction costs, one cannot in general efficiently internalize an externality by taxing/subsidizing whoever is generating the negative/positive externalities, because it would generally not result in the right to the resource affected going to the person who values it the most. | McCloskey: In other words, whoever is generating the negative/positive externalities is not a meaningful notion. In the railway/farmers example: who “causes” the burnt of fields of corn, the railway which makes the sparks, or the farmers who plant imprudently close to the line? | In the noise pollution around airports example: are the airplanes the “cause” of the pollution, or the ears that near the airports? The presence of ears is just as much a “cause” as the vibrations in the planes’ motors. Where then should the Pigou/Samuelson tax be placed? | . | Weisskopf: Is it true that Coase implies not that state should get out of the business of dealing with externalities, but implies that states should try to get the entitlements right rather than getting the prices right? | McCloskey: Yes. But Coase also claims, with considerable empirical evidence, that in many cases, government trying to get the entitlements right is worse than laissez faire (FCC or the lighthouses or the law of liability). In most cases, because of lack of knowledge. | McCloskey: In other words, Coase claims that laissez faire is desirable not because it’s the First Best (transaction costs are low, things will come into places), but because it is the Second Best (the transaction costs are so high that entitlements matters, and transaction costs are so high that it is hard for the government to get the entitlements right). | McCloskey: It is this claim, that lead people to confuse Coase as yet another laissez faire economist. | . | In a wider context, Pigou, Samuelson, and Stiger are “modernists”. Modernism has had some good moments, such as the Ronchamp Chapel by Le Corbusier or The Foundations of Economic Analysis by Paul Samuelson. It was worth a try, though on the whole it did not work very well. Have a look at economic policy; or, if that doesn’t appall you, then look at the average academic article in economics, pure theoretical modernism, blackboard economics gone loco. | I commend Coase for his old-fashioned ways. The old-fashioned ways have become the latest fashion, actually. You read it here: Ronald Coase is a postmodern economist. And his theorem, a post-modern one, is about the difficulty of knowing what is to be done. | .",
            "url": "https://yimingli.net/notes-mccloskey-on-coase-theorem.html",
            "relUrl": "/notes-mccloskey-on-coase-theorem.html",
            "date": " • Jun 18, 2013"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I am a Research Scientist working on privacy-preserving technologies, causal inference, and experimentation platforms at Facebook. . I hold a Ph.D. in Public Policy from the University of Chicago, where I studied Labor Economics, Public Economics, and Applied Microeconometrics. This website is powered by fastpages. .",
          "url": "https://yimingli.net/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yimingli.net/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}